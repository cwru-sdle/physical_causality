{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "print(f\"Repository Root: {repo_root}\")\n",
    "from models.mit_b2 import MIT_B2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSpaceAnalyzer:\n",
    "    def __init__(self, encoder_name='mit_b2', num_phases=3, in_channels=1,\n",
    "                 cache_dir='./weight_cache'):\n",
    "        \n",
    "        self.encoder_name = encoder_name\n",
    "        self.num_phases = num_phases\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.models = {}\n",
    "        self.weight_vectors = {}\n",
    "        \n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "    # -------------------------------\n",
    "    # Cache helpers\n",
    "    # -------------------------------\n",
    "    def _cache_path(self, model_name):\n",
    "        return self.cache_dir / f\"{model_name}.pkl\"\n",
    "    \n",
    "    def save_vec(self, name, v):\n",
    "        with open(self._cache_path(name), 'wb') as f:\n",
    "            pickle.dump(v, f)\n",
    "    \n",
    "    def load_vec(self, name):\n",
    "        p = self._cache_path(name)\n",
    "        if p.exists():\n",
    "            with open(p, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Loading models\n",
    "    # -------------------------------\n",
    "    def load_imagenet_base(self, use_cache=True):\n",
    "        name = \"W_img\"\n",
    "        cached = self.load_vec(name) if use_cache else None\n",
    "        if cached is not None:\n",
    "            self.weight_vectors[name] = cached\n",
    "            print(f\"Loaded {name} from cache.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading ImageNet pretrained model...\")\n",
    "        model = MIT_B2(\n",
    "            encoder_name=self.encoder_name,\n",
    "            num_phases=self.num_phases,\n",
    "            in_channels=self.in_channels,\n",
    "            pretrained='imagenet'\n",
    "        )\n",
    "        self.models[name] = model\n",
    "    \n",
    "    def load_checkpoint(self, ckpt_path, name, use_cache=True):\n",
    "        cached = self.load_vec(name) if use_cache else None\n",
    "        if cached is not None:\n",
    "            self.weight_vectors[name] = cached\n",
    "            print(f\"Loaded {name} from cache.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Loading {name} from:\\n  {ckpt_path}\")\n",
    "        model = MIT_B2.load_from_checkpoint(ckpt_path)\n",
    "        self.models[name] = model\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Extract encoder weights\n",
    "    # -------------------------------\n",
    "    def extract_encoder(self, model):\n",
    "        vecs = []\n",
    "        for p_name, param in model.named_parameters():\n",
    "            if \"encoder\" in p_name:\n",
    "                vecs.append(param.detach().cpu().flatten())\n",
    "        return torch.cat(vecs).numpy()\n",
    "    \n",
    "    def compute_weight_vectors(self, use_cache=True):\n",
    "        for name, model in list(self.models.items()):\n",
    "            print(f\"Extracting encoder vector for {name}...\")\n",
    "            w = self.extract_encoder(model)\n",
    "            self.weight_vectors[name] = w\n",
    "            \n",
    "            if use_cache:\n",
    "                self.save_vec(name, w)\n",
    "            \n",
    "            del self.models[name]   # free memory\n",
    "        \n",
    "        print(f\"✓ Extracted {len(self.weight_vectors)} weight vectors.\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # L2 Matrix\n",
    "    # -------------------------------\n",
    "    def compute_l2_matrix(self):\n",
    "        names = list(self.weight_vectors.keys())\n",
    "        n = len(names)\n",
    "        M = np.zeros((n, n))\n",
    "        \n",
    "        print(\"Computing L2 pairwise distances...\")\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                M[i,j] = np.linalg.norm(self.weight_vectors[names[i]] -\n",
    "                                        self.weight_vectors[names[j]])\n",
    "        return pd.DataFrame(M, index=names, columns=names)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Plot Heatmap\n",
    "    # -------------------------------\n",
    "    def plot_l2_matrix(self, df, save_path=None):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(df, annot=True, fmt=\".1f\", cmap=\"RdYlBu_r\",\n",
    "                    square=True, cbar_kws={\"label\": \"L2 Distance\"})\n",
    "        \n",
    "        plt.title(\"Pairwise L2 Distance Matrix (Encoder Weights Only)\", fontsize=16)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_paths = {\n",
    "    \"W_syn\": os.path.join(repo_root, \"output\", \"checkpoints\", \"Synthetic-PreTrained.ckpt\"),\n",
    "    \"W_real_direct_1\": os.path.join(repo_root, \"output\", \"checkpoints\", \"Dataset1-FineTuned-ImageNet-Initialization.ckpt\"),\n",
    "    \"W_real_via_syn_1\": os.path.join(repo_root, \"output\", \"checkpoints\", \"Dataset1-FineTuned-Synthetic-PreTrained.ckpt\"),\n",
    "    \"W_real_direct_2\": os.path.join(repo_root, \"output\", \"checkpoints\", \"Dataset2-FineTuned-ImageNet-Initialization.ckpt\"),\n",
    "    \"W_real_via_syn_2\": os.path.join(repo_root, \"output\", \"checkpoints\", \"Dataset2-FineTuned-Synthetic-PreTrained.ckpt\")\n",
    "}\n",
    "\n",
    "checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = WeightSpaceAnalyzer(cache_dir=\"./l2_cache\")\n",
    "\n",
    "# Step 1 – Load ImageNet\n",
    "an.load_imagenet_base()\n",
    "\n",
    "# Step 2 – Load all checkpoints\n",
    "for name, path in checkpoint_paths.items():\n",
    "    an.load_checkpoint(path, name)\n",
    "\n",
    "# Step 3 – Extract weight vectors\n",
    "an.compute_weight_vectors()\n",
    "\n",
    "# Step 4 – Compute L2 matrix\n",
    "df_l2 = an.compute_l2_matrix()\n",
    "df_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an.plot_l2_matrix(df_l2, save_path=\"l2_matrix.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
