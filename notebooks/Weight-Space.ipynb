{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "repo_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "print(f\"Repository Root: {repo_root}\")\n",
    "from models.mit_b2 import MIT_B2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSpaceAnalyzer:\n",
    "    def __init__(self, encoder_name='mit_b2', num_phases=3, in_channels=1,\n",
    "                 cache_dir='./weight_cache'):\n",
    "        \n",
    "        self.encoder_name = encoder_name\n",
    "        self.num_phases = num_phases\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.models = {}\n",
    "        self.weight_vectors = {}\n",
    "        \n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "    # -------------------------------\n",
    "    # Cache helpers\n",
    "    # -------------------------------\n",
    "    def _cache_path(self, model_name):\n",
    "        return self.cache_dir / f\"{model_name}.pkl\"\n",
    "    \n",
    "    def save_vec(self, name, v):\n",
    "        with open(self._cache_path(name), 'wb') as f:\n",
    "            pickle.dump(v, f)\n",
    "    \n",
    "    def load_vec(self, name):\n",
    "        p = self._cache_path(name)\n",
    "        if p.exists():\n",
    "            with open(p, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return None\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Loading models\n",
    "    # -------------------------------\n",
    "    def load_imagenet_base(self, use_cache=True):\n",
    "        name = \"W_img\"\n",
    "        cached = self.load_vec(name) if use_cache else None\n",
    "        if cached is not None:\n",
    "            self.weight_vectors[name] = cached\n",
    "            print(f\"Loaded {name} from cache.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Loading ImageNet pretrained model...\")\n",
    "        model = MIT_B2(\n",
    "            encoder_name=self.encoder_name,\n",
    "            num_phases=self.num_phases,\n",
    "            in_channels=self.in_channels,\n",
    "            pretrained='imagenet'\n",
    "        )\n",
    "        self.models[name] = model\n",
    "    \n",
    "    def load_checkpoint(self, ckpt_path, name, use_cache=True):\n",
    "        cached = self.load_vec(name) if use_cache else None\n",
    "        if cached is not None:\n",
    "            self.weight_vectors[name] = cached\n",
    "            print(f\"Loaded {name} from cache.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Loading {name} from:\\n  {ckpt_path}\")\n",
    "        model = MIT_B2.load_from_checkpoint(ckpt_path)\n",
    "        self.models[name] = model\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Extract encoder weights\n",
    "    # -------------------------------\n",
    "    def extract_encoder(self, model):\n",
    "        vecs = []\n",
    "        for p_name, param in model.named_parameters():\n",
    "            if \"encoder\" in p_name:\n",
    "                vecs.append(param.detach().cpu().flatten())\n",
    "        return torch.cat(vecs).numpy()\n",
    "    \n",
    "    def compute_weight_vectors(self, use_cache=True):\n",
    "        for name, model in list(self.models.items()):\n",
    "            print(f\"Extracting encoder vector for {name}...\")\n",
    "            w = self.extract_encoder(model)\n",
    "            self.weight_vectors[name] = w\n",
    "            \n",
    "            if use_cache:\n",
    "                self.save_vec(name, w)\n",
    "            \n",
    "            del self.models[name]   # free memory\n",
    "        \n",
    "        print(f\"✓ Extracted {len(self.weight_vectors)} weight vectors.\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    # L2 Matrix\n",
    "    # -------------------------------\n",
    "    def compute_l2_matrix(self):\n",
    "        names = list(self.weight_vectors.keys())\n",
    "        n = len(names)\n",
    "        M = np.zeros((n, n))\n",
    "        \n",
    "        print(\"Computing L2 pairwise distances...\")\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                M[i,j] = np.linalg.norm(self.weight_vectors[names[i]] -\n",
    "                                        self.weight_vectors[names[j]])\n",
    "        return pd.DataFrame(M, index=names, columns=names)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Plot Heatmap\n",
    "    # -------------------------------\n",
    "    def plot_l2_matrix(self, df, save_path=None):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(df, annot=True, fmt=\".1f\", cmap=\"RdYlBu_r\",\n",
    "                    square=True, cbar_kws={\"label\": \"L2 Distance\"})\n",
    "        \n",
    "        plt.title(\"Pairwise L2 Distance Matrix (Encoder Weights Only)\", fontsize=16)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_checkpoint(filename, osf_project_id=None):\n",
    "    \"\"\"\n",
    "    Ensure checkpoint exists locally, download from OSF if needed.\n",
    "    \n",
    "    Args:\n",
    "        filename: Exact checkpoint filename\n",
    "        osf_project_id: OSF project ID for downloading\n",
    "    \n",
    "    Returns:\n",
    "        Path to local checkpoint file\n",
    "    \"\"\"\n",
    "    ckpt_dir = os.path.join(repo_root, \"output\", \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    \n",
    "    local_path = os.path.join(ckpt_dir, filename)\n",
    "    \n",
    "    # If file exists locally, return it\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"Found locally: {filename}\")\n",
    "        return local_path\n",
    "    \n",
    "    # Otherwise, attempt to download from OSF\n",
    "    if osf_project_id:\n",
    "        print(f\"Downloading {filename} from OSF...\")\n",
    "        downloaded_path = download_from_osf(filename, ckpt_dir, osf_project_id)\n",
    "        if downloaded_path:\n",
    "            return downloaded_path\n",
    "    \n",
    "    print(f\"Checkpoint not found: {filename}\")\n",
    "    return local_path\n",
    "\n",
    "def download_from_osf(filename, ckpt_dir, osf_project_id):\n",
    "    \"\"\"\n",
    "    Download specific checkpoint file from OSF storage.\n",
    "    \n",
    "    Args:\n",
    "        filename: Exact filename to download\n",
    "        ckpt_dir: Local directory to save checkpoint\n",
    "        osf_project_id: OSF project ID\n",
    "    \n",
    "    Returns:\n",
    "        Path to downloaded file or None if download failed\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    # Query OSF API for available files\n",
    "    api_url = f\"https://api.osf.io/v2/nodes/{osf_project_id}/files/osfstorage/\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        files = response.json()['data']\n",
    "        \n",
    "        # Locate matching file\n",
    "        matching_file = None\n",
    "        for file in files:\n",
    "            if file['attributes']['name'] == filename:\n",
    "                matching_file = file\n",
    "                break\n",
    "        \n",
    "        if not matching_file:\n",
    "            print(f\"File '{filename}' not found on OSF\")\n",
    "            return None\n",
    "        \n",
    "        # Download file\n",
    "        download_url = matching_file['links']['download']\n",
    "        local_path = os.path.join(ckpt_dir, filename)\n",
    "        \n",
    "        response = requests.get(download_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 8192\n",
    "        \n",
    "        with open(local_path, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=block_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {percent:.1f}%\", end='')\n",
    "        \n",
    "        print(f\"\\nDownload complete: {filename}\")\n",
    "        return local_path\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading from OSF: {e}\")\n",
    "        return None\n",
    "\n",
    "# OSF project ID where checkpoint files are stored\n",
    "OSF_PROJECT_ID = \"g364t\"\n",
    "\n",
    "# Define checkpoint filenames\n",
    "checkpoint_files = {\n",
    "    \"W_syn\": \"Synthetic-PreTrained.ckpt\",\n",
    "    \"W_real_direct_1\": \"Dataset1-FineTuned-ImageNet-Initialization.ckpt\",\n",
    "    \"W_real_via_syn_1\": \"Dataset1-FineTuned-Synthetic-PreTrained.ckpt\",\n",
    "    \"W_real_direct_2\": \"Dataset2-FineTuned-ImageNet-Initialization.ckpt\",\n",
    "    \"W_real_via_syn_2\": \"Dataset2-FineTuned-Synthetic-PreTrained.ckpt\"\n",
    "}\n",
    "\n",
    "# Verify all checkpoints are available locally or download from OSF\n",
    "print(\"Checking checkpoints...\")\n",
    "checkpoint_paths = {\n",
    "    key: ensure_checkpoint(filename, osf_project_id=OSF_PROJECT_ID)\n",
    "    for key, filename in checkpoint_files.items()\n",
    "}\n",
    "\n",
    "print(\"\\nCheckpoint paths:\")\n",
    "for key, path in checkpoint_paths.items():\n",
    "    exists = \"exists\" if os.path.exists(path) else \"missing\"\n",
    "    print(f\"  [{exists}] {key}: {path}\")\n",
    "\n",
    "checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = WeightSpaceAnalyzer(cache_dir=\"./l2_cache\")\n",
    "\n",
    "# Step 1 – Load ImageNet\n",
    "an.load_imagenet_base()\n",
    "\n",
    "# Step 2 – Load all checkpoints\n",
    "for name, path in checkpoint_paths.items():\n",
    "    an.load_checkpoint(path, name)\n",
    "\n",
    "# Step 3 – Extract weight vectors\n",
    "an.compute_weight_vectors()\n",
    "\n",
    "# Step 4 – Compute L2 matrix\n",
    "df_l2 = an.compute_l2_matrix()\n",
    "df_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an.plot_l2_matrix(df_l2, save_path=\"l2_matrix.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
