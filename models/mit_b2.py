import torch
import torch.nn as nn
import torch.optim as optim
import pytorch_lightning as pl
import segmentation_models_pytorch as smp
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
from torchmetrics import MeanAbsoluteError, MeanSquaredError
import warnings

# Filter specific warnings typically generated by SMP or Metrics
warnings.filterwarnings("ignore", category=UserWarning, module='segmentation_models_pytorch')
warnings.filterwarnings("ignore", category=FutureWarning)

class MIT_B2(pl.LightningModule):
    def __init__(
        self, 
        encoder_name, 
        num_phases, 
        in_channels=1, 
        learning_rate=1e-4, 
        weight_decay=1e-5, 
        max_epochs=200,
        warmup_epochs=10,
        warmup_start_factor=0.1,
        pretrained='imagenet', 
        **encoder_kwargs
    ):
        """
        Args:
            encoder_name (str): Name of the encoder architecture (e.g., 'resnet34').
            num_phases (int): Number of phases to predict weight fractions for.
            in_channels (int): Number of input channels (1 for grayscale diffraction).
            learning_rate (float): Learning rate for the optimizer.
            pretrained (str): Pretrained weights to use ('imagenet' or None).
            **encoder_kwargs: Additional arguments for the smp encoder.
        """
        super().__init__()
        self.save_hyperparameters() # Saves args like encoder_name, num_phases, etc.

        # --- 1. Create the Encoder ---
        # We instantiate the full model just to easily grab the encoder and its preprocessing
        temp_model = smp.create_model(
            arch='unet', # We only need the encoder, architecture is unimportant
            encoder_name=encoder_name,
            in_channels=in_channels,
            classes=1, # Dummy value
            encoder_weights=pretrained,
            **encoder_kwargs
        )
        self.encoder = temp_model.encoder

        # --- 2. Preprocessing Parameters --- 
        params = smp.encoders.get_preprocessing_params(encoder_name, pretrained=pretrained)
        
        # Ensure std and mean tensors match in_channels correctly
        if in_channels == 1 and len(params["mean"]) == 3:
            mean = torch.tensor(params["mean"]).mean()
            std = torch.tensor(params["std"]).mean()
        else:
            mean = torch.tensor(params["mean"][:in_channels])
            std = torch.tensor(params["std"][:in_channels])

        # Reshape for broadcasting: (1, C, 1, 1)
        self.register_buffer("mean", mean.view(1, in_channels, 1, 1))
        self.register_buffer("std", std.view(1, in_channels, 1, 1))

        # --- 3. Regression Head ---
        # Determine the number of output features from the encoder dynamically
        dummy_input = torch.randn(1, in_channels, 64, 64) # Small dummy input
        
        with torch.no_grad():
            encoder_output = self.encoder(dummy_input)
            last_stage_features = encoder_output[-1]
            num_encoder_features = last_stage_features.shape[1] 

        # Create the regression head
        self.regression_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(output_size=1), # Global Average Pooling
            nn.Flatten(),                      
            nn.Linear(num_encoder_features, num_encoder_features // 2),
            nn.ReLU(),
            nn.Dropout(0.25), 
            nn.Linear(num_encoder_features // 2, num_phases) 
        )

        # --- 4. Loss Function ---
        self.loss_fn = nn.MSELoss()

        def make_metrics():
            return nn.ModuleDict({
                'mae': MeanAbsoluteError(),
                'mse': MeanSquaredError()
            })
            
        self.train_metrics = make_metrics()
        self.val_metrics = make_metrics()
        self.test_metrics = make_metrics()

    def forward(self, image):
        image = (image - self.mean) / self.std
        
        features = self.encoder(image)
        last_stage_features = features[-1]

        fractions = self.regression_head(last_stage_features)
        return fractions

    def _common_step(self, batch, batch_idx, stage):
        image, target_fractions = batch
        
        # Basic shape checks
        assert image.ndim == 4
        assert target_fractions.ndim == 2
        
        predicted_fractions = self.forward(image)
        loss = self.loss_fn(predicted_fractions, target_fractions)
        
        self.log(f'{stage}_loss', loss, on_step=True, on_epoch=True, logger=True, prog_bar=True, sync_dist=True)

        # Select metrics for the current stage
        metrics = {
            'train': self.train_metrics,
            'val': self.val_metrics,
            'test': self.test_metrics,
        }[stage]

        for name, metric in metrics.items():
            value = metric(predicted_fractions, target_fractions)
            self.log(f'{stage}_{name}', value, on_step=False, on_epoch=True, prog_bar=True)

        return {'loss': loss, 'preds': predicted_fractions.detach(), 'targets': target_fractions.detach()}

    def training_step(self, batch, batch_idx):
        return self._common_step(batch, batch_idx, "train")

    def validation_step(self, batch, batch_idx):
        return self._common_step(batch, batch_idx, "val")

    def test_step(self, batch, batch_idx):
        return self._common_step(batch, batch_idx, "test")

    def configure_optimizers(self):
        """Sets up the AdamW optimizer and LR scheduler with linear warmup."""
        optimizer = optim.AdamW(
            self.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=self.hparams.weight_decay
        )

        # 1. Linear Warmup Scheduler
        warmup_scheduler = LinearLR(
            optimizer,
            start_factor=self.hparams.warmup_start_factor,
            end_factor=1.0,
            total_iters=self.hparams.warmup_epochs
        )

        # 2. Main Decay Scheduler
        decay_epochs = self.hparams.max_epochs - self.hparams.warmup_epochs
        main_scheduler = CosineAnnealingLR(
            optimizer,
            T_max=decay_epochs if decay_epochs > 0 else 1,
            eta_min=self.hparams.learning_rate * 0.01 
        )

        # 3. Chain the Schedulers
        scheduler = SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[self.hparams.warmup_epochs]
        )

        lr_scheduler_config = {
            "scheduler": scheduler,
            "interval": "epoch",
            "frequency": 1,
            "name": "LinearWarmup_CosineAnnealingLR"
        }

        return {"optimizer": optimizer, "lr_scheduler": lr_scheduler_config}